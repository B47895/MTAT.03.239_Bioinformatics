---
title: "Simulating random effects"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load the necessary packages
```{r}
library("lme4")
library("dplyr")
library("ggplot2")
```

# Application to a balanced dataset

## Simulate data

Simulate a dataset with 5 individuals, each one containing 10 replicates.

Specify tesired desired between-individual variance and measurement noise variance. In this case the variance between individuals equal to the variance within individuals.

```{r}
var_individual = 0.5
var_noise = 0.5
set.seed(34)
```

We can confirm this by calculating the theoretical (population) variance explained by between-individual differences:
```{r}
var_individual/(var_individual + var_noise)
```

Now, let's simulate the data:
```{r}
#Specify the number of replicates per individual
replicate_counts = c(10,10,10,10,10)

#Sample individual means from the normal distribution with variance var_individual
individual_means = rnorm(5, mean = 0, sd = sqrt(var_individual))
ind_means = dplyr::tibble(individual_mean = individual_means, 
                              individual = paste0("ind_", c(1:5)))

#Add replicates and sample measurement noise
data_balanced = dplyr::tibble(individual = paste0("ind_", rep(c(1,2,3,4,5), times = replicate_counts))) %>%
  dplyr::left_join(ind_means, by = "individual") %>%
  #Sample measurement noise from the normal distribution with variance var_noise
  dplyr::mutate(noise = rnorm(length(individual), mean = 0, sd = sqrt(var_noise))) %>%
  dplyr::mutate(expression = individual_mean + noise) %>%
  dplyr::mutate(sample_index = c(1:length(individual)))
data_balanced
```


Simulated sample variance explained by between-individual differences. Note that this is not exactly same as the theortical estimate above due to sampling noise. 
```{r}
var(individual_means)/(var(individual_means) + var(data_balanced$noise))
```

## Visualise data

Make a plot of the raw data
```{r}
ggplot(data_balanced, aes(x = sample_index, y = expression, color = individual)) + 
  geom_point() + 
  facet_grid(~individual, scales = "free_x")
```

Specify a helper function that extracts the proportion of variance explained by different paramters of the linear mixed model
```{r}
#' Calculate the proportion of variance explaned by different factors in a lme4 model
varianceExplained <- function(lmer_model){
  variance = as.data.frame(lme4::VarCorr(lmer_model))
  var_percent = dplyr::mutate(variance, percent_variance = vcov/sum(vcov)) %>% 
    dplyr::select(grp, percent_variance) %>% 
    dplyr::mutate(type = "gene")
  var_row = tidyr::spread(var_percent, grp, percent_variance)
  return(var_row)  
}
```

Fit a linear mixed model to the dataset to estimate the porportion of variance explained by differences between individuals. Note that this model assumes that the underlying "true" mean expression level for each individual comes from the same distribution. 
```{r}
model = lmer(expression ~ (1|individual), data_balanced)
varianceExplained(model)
```

Perform the same analysis with a standard fixed effect linear model. The standard fixed effect linear model considers each individual completely seprately, without taking other individuals into account.
```{r}
model_fixed = lm(expression ~ individual, data_balanced)
#Estimate variance explained
summary(model_fixed)$adj.r.squared
```

Extract model coefficients (estimated mean expression levels for each individual) from the linear mixed model:
```{r}
coefs = coef(model)$individual
coefs_df = dplyr::data_frame(individual = rownames(coefs), coef = coefs[,1], type = "lme4")
```

Estmate individual means for the linear model (these are just the means calculated for each individual separately):
```{r}
ind_means = dplyr::group_by(data_balanced, individual) %>% 
  dplyr::summarize(coef = mean(expression), type = "lm")
```

Visualise the raw data together with the estimated means from the linear model and linear mixed model:
```{r}
ggplot(data_balanced, aes(x = sample_index, y = expression, color = individual)) + 
  geom_point() + 
  facet_grid(~individual, scales = "free_x") + 
  geom_hline(data = coefs_df, aes(yintercept = coef, linetype = type)) +
  geom_hline(data = ind_means, aes(yintercept = coef, linetype = type))
```

What you should see is that although the estimates from the linear model (lm) and linear mixed model (lme4) are similar, they are not exactly the same. This is because we have simulated a relatively large measurement noise (var_noise) which makes the linear mixed model to "trust" the estimates within each individual less and forces the estimates from each individual to be close to the global mean across individuals.


# Application to an unbalanced dataset

Now, let's consider an unbalanced example where the number of samples collected from each individual is very different. For example, here we have a situation where three individuals have very many replicates (13-20) and two individuals have only two replicates:

```{r}
#Specify the number of replicates per individual
replicate_counts = c(20,15,13,1,1)

#Sample individual means from the normal distribution with variance var_individual
individual_means = rnorm(5, mean = 0, sd = sqrt(var_individual))
ind_means = dplyr::data_frame(individual_mean = individual_means, 
                              individual = paste0("ind_", c(1:5)))

#Add replicates and sample measurement noise
data_unbalanced = dplyr::data_frame(individual = paste0("ind_", rep(c(1,2,3,4,5), times = replicate_counts))) %>%
  dplyr::left_join(ind_means, by = "individual") %>%
  #Sample measurement noise from the normal distribution with variance var_noise
  dplyr::mutate(noise = rnorm(length(individual), mean = 0, sd = sqrt(var_noise))) %>%
  dplyr::mutate(expression = individual_mean + noise) %>%
  dplyr::mutate(sample_index = c(1:length(individual)))
data_unbalanced
```

Visualise the data
```{r}
ggplot(data_unbalanced, aes(x = sample_index, y = expression, color = individual)) + 
  geom_point() + 
  facet_grid(~individual, scales = "free_x")
```

Estimate simulated variance explained
```{r}
var(individual_means)/(var(individual_means) + var(data_unbalanced$noise))
```

With linear mixed model
```{r}
model = lmer(expression ~ (1|individual), data_unbalanced)
varianceExplained(model)
```

With linear model
```{r}
model_fixed = lm(expression ~ individual, data_unbalanced)
#Estimate variance explained
summary(model_fixed)$adj.r.squared
```

Extract model (estimated mean expression levels for each individual) from the linear mixed model:
```{r}
coefs = coef(model)$individual
coefs_df = dplyr::data_frame(individual = rownames(coefs), coef = coefs[,1], type = "lme4")
model_df = dplyr::left_join(data_unbalanced, coefs_df, by = "individual")
```

Estmate individual means for the linear model (these are just the means calculated for each individual separately):
```{r}
ind_means = dplyr::group_by(data_unbalanced, individual) %>% 
  dplyr::summarize(coef = mean(expression), type = "lm")
```

Visualise the data together with estimated means. 
```{r}
ggplot(data_unbalanced, aes(x = sample_index, y = expression, color = individual)) + 
  geom_point() + 
  facet_grid(~individual, scales = "free_x") + 
  geom_hline(data = coefs_df, aes(yintercept = coef, linetype = type)) +
  geom_hline(data = ind_means, aes(yintercept = coef, linetype = type))
```

From here, you should be able to see that for individuals 1-3 that have many data points both linear model and linear mixed model produce very similar estimates for mean expression. However, for individuals 4 and 5 that each have only one data point, the only estimate that a linear model can provide for the mean is the value of the one data point itself. In contrast, because the linear mixed model assumes that all individuals come the from same global distribution with shared mean and standard deviation, it "pulls" the estimates for individuals 4 and 5 closer to the means of the other individuals, because it recognises that there is less data to support such large mean values for these two individuals.

## Exercise 1
You should now try to change the `var_noise` parameter at the top of this document from 0.5 to 0.05, thus reducing the simulated measurement noise by 10-fold. What do you see? Are the estimates from the linear mixed model and linear model now agreeing more with each other? 

